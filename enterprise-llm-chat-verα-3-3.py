#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Quantum-Enhanced Self-Evolving Enterprise LLM System v3.0Î³
è¶…é«˜åº¦AIä¼šè©±ã‚·ã‚¹ãƒ†ãƒ  - é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ»è‡ªå·±é€²åŒ–ãƒ»åˆ†æ•£æ¨è«–

ğŸš€ é©æ–°çš„æ©Ÿèƒ½:
- ğŸ”® Quantum-Inspired Optimization (QAOAé¢¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ )
- ğŸ§¬ Genetic Algorithm for Prompt Evolution
- ğŸŒŠ Swarm Intelligence for Multi-Agent Coordination
- ğŸ­ Multi-Persona Debate System
- ğŸ”¬ Automated Hypothesis Generation & Testing
- ğŸ“Š Bayesian Confidence Calibration
- ğŸ¯ Reinforcement Learning from Human Feedback (RLHF)
- ğŸ§  Neural Architecture Search with Meta-Learning
- ğŸ”„ Self-Improving Reasoning Chains
- ğŸŒ Distributed Consensus Mechanism

ä½¿ã„æ–¹:
export GROQ_API_KEY='your_key'
pip install groq numpy scipy
python enterprise-llm-chat-verÎ³.py
"""

import os
import sys
import time
import json
import hashlib
import logging
import asyncio
import re
import uuid
import math
import statistics
from typing import Optional, List, Dict, Any, Callable, Tuple, Set, Union
from dataclasses import dataclass, field, asdict
from collections import defaultdict, Counter, deque
from datetime import datetime, timedelta
from enum import Enum
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from functools import lru_cache

import numpy as np

try:
    from groq import Groq, RateLimitError, APIError
except ImportError:
    print("âŒ Required: pip install groq numpy scipy")
    sys.exit(1)

try:
    import readline
except ImportError:
    pass

# ==================== å®šæ•°ãƒ»åˆ—æŒ™å‹ ====================

class Intent(str, Enum):
    QUESTION = "question"
    COMMAND = "command"
    CREATIVE = "creative"
    TECHNICAL = "technical"
    CASUAL = "casual"
    EXPLANATION = "explanation"
    REASONING = "reasoning"
    ANALYSIS = "analysis"
    RESEARCH = "research"
    PLANNING = "planning"
    DEBUGGING = "debugging"
    OPTIMIZATION = "optimization"


class Complexity(str, Enum):
    TRIVIAL = "trivial"
    SIMPLE = "simple"
    MEDIUM = "medium"
    COMPLEX = "complex"
    EXPERT = "expert"
    RESEARCH = "research"
    FRONTIER = "frontier"


class Strategy(str, Enum):
    DIRECT = "direct"
    COT = "chain_of_thought"
    REFLECTION = "reflection"
    ENSEMBLE = "ensemble"
    ITERATIVE = "iterative"
    TREE_SEARCH = "tree_search"
    DEBATE = "debate"
    SYNTHESIS = "synthesis"
    SWARM = "swarm_intelligence"
    GENETIC = "genetic_evolution"
    QUANTUM = "quantum_inspired"


class PersonaType(str, Enum):
    OPTIMIST = "optimist"
    PESSIMIST = "pessimist"
    PRAGMATIST = "pragmatist"
    INNOVATOR = "innovator"
    CRITIC = "critic"
    SYNTHESIZER = "synthesizer"


class ReasoningType(str, Enum):
    DEDUCTIVE = "deductive"
    INDUCTIVE = "inductive"
    ABDUCTIVE = "abductive"
    ANALOGICAL = "analogical"
    CAUSAL = "causal"
    COUNTERFACTUAL = "counterfactual"
    BAYESIAN = "bayesian"


# ==================== è¨­å®š ====================

@dataclass
class QuantumConfig:
    """é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢è¨­å®š"""
    enabled: bool = True
    num_qubits: int = 8
    iterations: int = 10
    optimization_depth: int = 3


@dataclass
class GeneticConfig:
    """éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¨­å®š"""
    enabled: bool = True
    population_size: int = 20
    mutation_rate: float = 0.15
    crossover_rate: float = 0.7
    elite_ratio: float = 0.2
    generations: int = 5


@dataclass
class SwarmConfig:
    """ç¾¤çŸ¥èƒ½è¨­å®š"""
    enabled: bool = True
    num_agents: int = 5
    inertia_weight: float = 0.7
    cognitive_weight: float = 1.5
    social_weight: float = 1.5
    max_iterations: int = 10


@dataclass
class RLHFConfig:
    """RLHFè¨­å®š"""
    enabled: bool = True
    learning_rate: float = 0.01
    discount_factor: float = 0.95
    exploration_rate: float = 0.1
    reward_shaping: bool = True


@dataclass
class SystemConfig:
    """ã‚·ã‚¹ãƒ†ãƒ è¨­å®š"""
    # åŸºæœ¬è¨­å®š
    model: str = "llama-3.1-8b-instant"
    max_tokens: int = 4000
    temperature: float = 0.7
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ»DB
    vec_db: bool = True
    vec_dim: int = 384
    cache_ttl: int = 3600
    similarity_threshold: float = 0.92
    
    # ãƒªãƒˆãƒ©ã‚¤
    max_retries: int = 3
    retry_delay: float = 1.0
    max_query_length: int = 15000
    
    # ã‚³ã‚¢æ©Ÿèƒ½
    adaptive: bool = True
    multi_armed_bandit: bool = True
    long_term_memory: bool = True
    knowledge_graph: bool = True
    chain_of_thought: bool = True
    self_reflection: bool = True
    ensemble_learning: bool = True
    metacognition: bool = True
    
    # é«˜åº¦ãªæ©Ÿèƒ½
    tree_of_thoughts: bool = True
    debate_mode: bool = True
    critic_system: bool = True
    confidence_calibration: bool = True
    active_learning: bool = True
    curriculum_learning: bool = True
    
    # è¶…é«˜åº¦ãªæ©Ÿèƒ½
    quantum: QuantumConfig = field(default_factory=QuantumConfig)
    genetic: GeneticConfig = field(default_factory=GeneticConfig)
    swarm: SwarmConfig = field(default_factory=SwarmConfig)
    rlhf: RLHFConfig = field(default_factory=RLHFConfig)
    
    # å®Ÿé¨“çš„æ©Ÿèƒ½
    hypothesis_testing: bool = True
    self_improvement: bool = True
    distributed_reasoning: bool = True
    neural_architecture_search: bool = True
    multi_modal_fusion: bool = False


# ==================== ãƒ‡ãƒ¼ã‚¿æ§‹é€  ====================

@dataclass
class Response:
    """LLMå¿œç­”"""
    text: str
    confidence: float
    tokens: int = 0
    prompt_tokens: int = 0
    completion_tokens: int = 0
    latency: float = 0
    cost: float = 0
    model: str = ""
    timestamp: datetime = field(default_factory=datetime.now)
    finish_reason: str = "unknown"
    cached: bool = False
    similarity: float = 0
    rating: Optional[int] = None
    
    # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    intent: Optional[Intent] = None
    complexity: Optional[Complexity] = None
    sentiment: float = 0
    strategy: Optional[Strategy] = None
    reasoning_type: Optional[ReasoningType] = None
    reasoning_steps: List[str] = field(default_factory=list)
    reflection: Optional[str] = None
    uncertainty: float = 0
    alternatives: List[Dict] = field(default_factory=list)
    
    # å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹
    coherence_score: float = 0
    relevance_score: float = 0
    completeness_score: float = 0
    factuality_score: float = 0
    novelty_score: float = 0
    
    # é«˜åº¦ãªãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
    bayesian_confidence: Optional[Tuple[float, float]] = None  # (mean, std)
    hypothesis_tested: List[str] = field(default_factory=list)
    personas_involved: List[str] = field(default_factory=list)
    genetic_fitness: float = 0
    quantum_optimized: bool = False
    swarm_consensus: float = 0
    
    @property
    def success(self) -> bool:
        return self.finish_reason in ("stop", "length")
    
    @property
    def quality_score(self) -> float:
        """ç·åˆå“è³ªã‚¹ã‚³ã‚¢"""
        scores = [
            self.confidence * 0.25,
            self.coherence_score * 0.2,
            self.relevance_score * 0.25,
            self.completeness_score * 0.15,
            self.factuality_score * 0.15
        ]
        return sum(s for s in scores if s > 0)
    
    def to_dict(self) -> Dict:
        return {
            'text': self.text,
            'confidence': self.confidence,
            'quality_score': self.quality_score,
            'strategy': self.strategy.value if self.strategy else None,
            'complexity': self.complexity.value if self.complexity else None,
            'cost': self.cost,
            'tokens': self.tokens,
            'latency': self.latency
        }


@dataclass
class Prompt:
    """é€²åŒ–ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ"""
    id: str
    template: str
    category: str
    fitness: float = 0.5
    usage_count: int = 0
    success_count: int = 0
    avg_quality: float = 0.5
    generation: int = 0
    parent_id: Optional[str] = None
    mutations: int = 0
    genes: List[str] = field(default_factory=list)
    
    @property
    def success_rate(self) -> float:
        return self.success_count / self.usage_count if self.usage_count > 0 else 0.5
    
    def mutate(self, mutation_rate: float = 0.15) -> str:
        """éºä¼çš„å¤‰ç•°"""
        if np.random.random() > mutation_rate:
            return self.template
        
        mutations = [
            lambda t: t.replace("Explain", "Elaborate on"),
            lambda t: t.replace("provide", "deliver"),
            lambda t: t.replace("answer", "respond to"),
            lambda t: f"{t} Think step by step.",
            lambda t: f"{t} Consider multiple perspectives.",
            lambda t: f"Carefully {t.lower()}",
            lambda t: t.replace(".", " with specific examples."),
            lambda t: f"From first principles, {t.lower()}",
            lambda t: f"{t} Show your reasoning.",
            lambda t: t.replace("describe", "analyze in depth")
        ]
        
        mutated = np.random.choice(mutations)(self.template)
        self.mutations += 1
        return mutated
    
    @staticmethod
    def crossover(parent1: 'Prompt', parent2: 'Prompt') -> str:
        """äº¤å‰"""
        words1 = parent1.template.split()
        words2 = parent2.template.split()
        
        # å˜ä¸€ç‚¹äº¤å‰
        point = np.random.randint(1, min(len(words1), len(words2)))
        child_words = words1[:point] + words2[point:]
        
        return ' '.join(child_words)


@dataclass
class Agent:
    """ç¾¤çŸ¥èƒ½ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    id: str
    position: np.ndarray  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã§ã®ä½ç½®
    velocity: np.ndarray
    best_position: np.ndarray
    best_fitness: float = -float('inf')
    persona: PersonaType = PersonaType.PRAGMATIST
    
    def update_velocity(
        self,
        global_best_position: np.ndarray,
        w: float,
        c1: float,
        c2: float
    ):
        """é€Ÿåº¦æ›´æ–°ï¼ˆPSOï¼‰"""
        r1, r2 = np.random.random(2)
        
        cognitive = c1 * r1 * (self.best_position - self.position)
        social = c2 * r2 * (global_best_position - self.position)
        
        self.velocity = w * self.velocity + cognitive + social
    
    def update_position(self):
        """ä½ç½®æ›´æ–°"""
        self.position = self.position + self.velocity
        # ç¯„å›²åˆ¶é™
        self.position = np.clip(self.position, 0, 1)


@dataclass
class Hypothesis:
    """ä»®èª¬"""
    id: str
    statement: str
    confidence: float
    evidence: List[str] = field(default_factory=list)
    counter_evidence: List[str] = field(default_factory=list)
    tested: bool = False
    result: Optional[bool] = None
    bayesian_prior: float = 0.5
    bayesian_posterior: float = 0.5


@dataclass
class KnowledgeNode:
    """çŸ¥è­˜ã‚°ãƒ©ãƒ•ãƒãƒ¼ãƒ‰"""
    id: str
    name: str
    type: str
    properties: Dict[str, Any] = field(default_factory=dict)
    embedding: Optional[np.ndarray] = None
    confidence: float = 1.0
    created: datetime = field(default_factory=datetime.now)
    updated: datetime = field(default_factory=datetime.now)
    access_count: int = 0
    relevance_score: float = 0.5


@dataclass
class KnowledgeEdge:
    """çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚¨ãƒƒã‚¸"""
    source: str
    target: str
    relation: str
    weight: float = 1.0
    confidence: float = 1.0
    properties: Dict[str, Any] = field(default_factory=dict)
    created: datetime = field(default_factory=datetime.now)


# ==================== ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ ====================

class Logger:
    """é«˜æ©Ÿèƒ½ãƒ­ã‚¬ãƒ¼"""
    
    def __init__(self, name: str, level: str = "INFO"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level))
        
        if not self.logger.handlers:
            handler = logging.StreamHandler(sys.stdout)
            formatter = logging.Formatter(
                '%(asctime)s | %(levelname)-8s | %(message)s',
                datefmt='%H:%M:%S'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
    
    def info(self, msg: str):
        self.logger.info(msg)
    
    def warning(self, msg: str):
        self.logger.warning(msg)
    
    def error(self, msg: str):
        self.logger.error(msg)
    
    def debug(self, msg: str):
        self.logger.debug(msg)


logger = Logger('quantum-llm')


class VectorDB:
    """é«˜åº¦ãªãƒ™ã‚¯ãƒˆãƒ«DB"""
    
    def __init__(self, dimension: int = 384):
        self.dimension = dimension
        self.vectors: List[Tuple[str, np.ndarray, Dict]] = []
        self.index_cache: Dict[str, int] = {}
    
    @lru_cache(maxsize=1000)
    def _embed(self, text: str) -> np.ndarray:
        """ãƒ†ã‚­ã‚¹ãƒˆã‚’åŸ‹ã‚è¾¼ã¿ãƒ™ã‚¯ãƒˆãƒ«ã«å¤‰æ›"""
        # ã‚·ãƒ³ãƒ—ãƒ«ãªãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹åŸ‹ã‚è¾¼ã¿ + TF-IDFé¢¨
        words = re.findall(r'\b\w+\b', text.lower())
        word_freq = Counter(words)
        
        hash_bytes = hashlib.sha256(text.encode()).digest()
        seed = int.from_bytes(hash_bytes[:4], 'little')
        rng = np.random.RandomState(seed)
        
        vec = rng.randn(self.dimension).astype(np.float32)
        
        # å˜èªé »åº¦ã§é‡ã¿ä»˜ã‘
        for word, freq in word_freq.most_common(10):
            word_seed = int.from_bytes(hashlib.md5(word.encode()).digest()[:4], 'little')
            word_rng = np.random.RandomState(word_seed)
            word_vec = word_rng.randn(self.dimension).astype(np.float32)
            vec += word_vec * (freq / len(words))
        
        norm = np.linalg.norm(vec)
        return vec / norm if norm > 0 else vec
    
    def add(self, id: str, text: str, metadata: Dict):
        """ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿½åŠ """
        embedding = self._embed(text)
        metadata = metadata or {}
        metadata['text'] = text
        metadata['added_at'] = time.time()
        
        self.index_cache[id] = len(self.vectors)
        self.vectors.append((id, embedding, metadata))
    
    def search(self, query: str, top_k: int = 5, min_similarity: float = 0.0) -> List[Tuple[str, float, Dict]]:
        """é¡ä¼¼æ¤œç´¢ï¼ˆé«˜é€ŸåŒ–ç‰ˆï¼‰"""
        if not self.vectors:
            return []
        
        query_vec = self._embed(query)
        
        # ãƒ™ã‚¯ãƒˆãƒ«åŒ–æ¼”ç®—ã§é«˜é€ŸåŒ–
        all_vecs = np.array([v[1] for v in self.vectors])
        similarities = np.dot(all_vecs, query_vec)
        
        # é–¾å€¤ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        valid_indices = np.where(similarities >= min_similarity)[0]
        
        if len(valid_indices) == 0:
            return []
        
        # ãƒˆãƒƒãƒ—Kå–å¾—
        sorted_indices = valid_indices[np.argsort(similarities[valid_indices])[::-1]][:top_k]
        
        results = [
            (self.vectors[i][0], float(similarities[i]), self.vectors[i][2])
            for i in sorted_indices
        ]
        
        return results
    
    def update_metadata(self, id: str, metadata: Dict):
        """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æ›´æ–°"""
        if id in self.index_cache:
            idx = self.index_cache[id]
            vec_id, vec, old_meta = self.vectors[idx]
            old_meta.update(metadata)
    
    def get_statistics(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±"""
        return {
            'total_vectors': len(self.vectors),
            'dimension': self.dimension,
            'cache_size': len(self._embed.cache_info()._asdict())
        }


# ==================== é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« ====================

class QuantumOptimizer:
    """é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢æœ€é©åŒ–å™¨"""
    
    def __init__(self, config: QuantumConfig):
        self.config = config
        self.num_qubits = config.num_qubits
    
    def optimize_parameters(
        self,
        objective_function: Callable[[np.ndarray], float],
        bounds: Tuple[float, float] = (0, 1)
    ) -> Tuple[np.ndarray, float]:
        """QAOAé¢¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æœ€é©åŒ–"""
        # åˆæœŸçŠ¶æ…‹: é‡ã­åˆã‚ã›ï¼ˆå‡ç­‰åˆ†å¸ƒï¼‰
        best_params = np.random.uniform(bounds[0], bounds[1], self.num_qubits)
        best_value = objective_function(best_params)
        
        for iteration in range(self.config.iterations):
            # é‡å­ã‚²ãƒ¼ãƒˆé¢¨ã®æ“ä½œ
            # 1. å›è»¢ã‚²ãƒ¼ãƒˆï¼ˆæ¢ç´¢ï¼‰
            rotation_angle = np.pi * (1 - iteration / self.config.iterations)
            candidate = best_params + np.random.randn(self.num_qubits) * rotation_angle * 0.1
            candidate = np.clip(candidate, bounds[0], bounds[1])
            
            # 2. ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆï¼ˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é–“ã®ç›¸é–¢ï¼‰
            if self.num_qubits > 1:
                for i in range(self.num_qubits - 1):
                    if np.random.random() < 0.3:
                        coupling = (candidate[i] + candidate[i + 1]) / 2
                        candidate[i] = candidate[i + 1] = coupling
            
            # 3. æ¸¬å®šï¼ˆè©•ä¾¡ï¼‰
            value = objective_function(candidate)
            
            # 4. æŒ¯å¹…å¢—å¹…ï¼ˆè‰¯ã„è§£ã‚’å¼·åŒ–ï¼‰
            if value > best_value:
                best_params = candidate
                best_value = value
                logger.debug(f"ğŸ”® Quantum iter {iteration}: improved to {value:.4f}")
        
        return best_params, best_value
    
    def quantum_annealing(
        self,
        energy_function: Callable[[np.ndarray], float],
        initial_state: np.ndarray,
        temperature_schedule: Optional[List[float]] = None
    ) -> np.ndarray:
        """é‡å­ã‚¢ãƒ‹ãƒ¼ãƒªãƒ³ã‚°é¢¨ã®æœ€é©åŒ–"""
        if temperature_schedule is None:
            temperature_schedule = np.logspace(0, -2, self.config.iterations)
        
        current_state = initial_state.copy()
        current_energy = energy_function(current_state)
        
        for temp in temperature_schedule:
            # éš£æ¥çŠ¶æ…‹ã‚’ç”Ÿæˆ
            neighbor = current_state + np.random.randn(len(current_state)) * temp
            neighbor = np.clip(neighbor, 0, 1)
            
            neighbor_energy = energy_function(neighbor)
            
            # ãƒ¡ãƒˆãƒ­ãƒãƒªã‚¹åŸºæº–
            delta_energy = neighbor_energy - current_energy
            if delta_energy < 0 or np.random.random() < np.exp(-delta_energy / temp):
                current_state = neighbor
                current_energy = neighbor_energy
        
        return current_state


# ==================== éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  ====================

class GeneticPromptEvolver:
    """éºä¼çš„ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«ã‚ˆã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé€²åŒ–"""
    
    def __init__(self, config: GeneticConfig):
        self.config = config
        self.population: List[Prompt] = []
        self.generation = 0
        self.best_ever: Optional[Prompt] = None
    
    def initialize_population(self, base_templates: List[str], category: str):
        """åˆæœŸé›†å›£ã‚’ç”Ÿæˆ"""
        self.population = []
        for i, template in enumerate(base_templates):
            prompt = Prompt(
                id=str(uuid.uuid4())[:8],
                template=template,
                category=category,
                generation=0,
                genes=template.split()
            )
            self.population.append(prompt)
        
        # è¿½åŠ ã§ãƒ©ãƒ³ãƒ€ãƒ å¤‰ç•°ä½“ã‚’ç”Ÿæˆ
        while len(self.population) < self.config.population_size:
            parent = np.random.choice(base_templates)
            mutated = self._mutate_template(parent)
            prompt = Prompt(
                id=str(uuid.uuid4())[:8],
                template=mutated,
                category=category,
                generation=0,
                mutations=1,
                genes=mutated.split()
            )
            self.population.append(prompt)
    
    def _mutate_template(self, template: str) -> str:
        """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆå¤‰ç•°"""
        mutations = [
            lambda t: t.replace("Explain", "Elaborate on"),
            lambda t: t.replace("provide", "give"),
            lambda t: f"{t} Think carefully.",
            lambda t: f"Step by step, {t.lower()}",
            lambda t: t.replace(".", " with examples."),
            lambda t: f"Considering multiple angles, {t.lower()}",
        ]
        return np.random.choice(mutations)(template)
    
    def evolve(self, fitness_evaluator: Callable[[Prompt], float]) -> Prompt:
        """ä¸€ä¸–ä»£é€²åŒ–"""
        self.generation += 1
        
        # é©å¿œåº¦è©•ä¾¡
        for prompt in self.population:
            if prompt.fitness == 0.5:  # æœªè©•ä¾¡
                prompt.fitness = fitness_evaluator(prompt)
        
        # ã‚½ãƒ¼ãƒˆ
        self.population.sort(key=lambda p: p.fitness, reverse=True)
        
        # ã‚¨ãƒªãƒ¼ãƒˆä¿å­˜
        elite_count = int(self.config.population_size * self.config.elite_ratio)
        new_population = self.population[:elite_count].copy()
        
        # æœ€è‰¯å€‹ä½“ã®è¿½è·¡
        if self.best_ever is None or self.population[0].fitness > self.best_ever.fitness:
            self.best_ever = self.population[0]
        
        # äº¤å‰ã¨å¤‰ç•°ã§æ–°å€‹ä½“ç”Ÿæˆ
        while len(new_population) < self.config.population_size:
            # è¦ªé¸æŠï¼ˆãƒˆãƒ¼ãƒŠãƒ¡ãƒ³ãƒˆé¸æŠï¼‰
            tournament_size = 3
            tournament = np.random.choice(self.population[:len(self.population)//2], tournament_size)
            parent1 = max(tournament, key=lambda p: p.fitness)
            
            tournament = np.random.choice(self.population[:len(self.population)//2], tournament_size)
            parent2 = max(tournament, key=lambda p: p.fitness)
            
            # äº¤å‰
            if np.random.random() < self.config.crossover_rate:
                child_template = Prompt.crossover(parent1, parent2)
            else:
                child_template = parent1.template
            
            # å¤‰ç•°
            if np.random.random() < self.config.mutation_rate:
                child_template = self._mutate_template(child_template)
            
            child = Prompt(
                id=str(uuid.uuid4())[:8],
                template=child_template,
                category=parent1.category,
                generation=self.generation,
                parent_id=parent1.id,
                genes=child_template.split()
            )
            
            new_population.append(child)
        
        self.population = new_population
        logger.info(f"ğŸ§¬ Generation {self.generation}: Best fitness = {self.population[0].fitness:.4f}")
        
        return self.population[0]
    
    def get_best_prompts(self, top_k: int = 3) -> List[Prompt]:
        """ä¸Šä½Kå€‹ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å–å¾—"""
        return sorted(self.population, key=lambda p: p.fitness, reverse=True)[:top_k]


# ==================== ç¾¤çŸ¥èƒ½ ====================

class SwarmIntelligence:
    """ç¾¤çŸ¥èƒ½ã‚·ã‚¹ãƒ†ãƒ ï¼ˆPSOï¼‰"""
    
    def __init__(self, config: SwarmConfig):
        self.config = config
        self.agents: List[Agent] = []
        self.global_best_position: Optional[np.ndarray] = None
        self.global_best_fitness: float = -float('inf')
        self.dimension = 5  # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ¬¡å…ƒï¼ˆtemp, top_p, frequency_penalty, etc.ï¼‰
    
    def initialize_swarm(self):
        """ç¾¤ã‚Œã‚’åˆæœŸåŒ–"""
        personas = list(PersonaType)
        self.agents = []
        
        for i in range(self.config.num_agents):
            position = np.random.random(self.dimension)
            velocity = np.random.randn(self.dimension) * 0.1
            
            agent = Agent(
                id=f"agent_{i}",
                position=position,
                velocity=velocity,
                best_position=position.copy(),
                persona=personas[i % len(personas)]
            )
            self.agents.append(agent)
    
    def optimize(
        self,
        fitness_function: Callable[[np.ndarray, PersonaType], float],
        max_iterations: Optional[int] = None
    ) -> Tuple[np.ndarray, float]:
        """ç¾¤æœ€é©åŒ–"""
        if not self.agents:
            self.initialize_swarm()
        
        iterations = max_iterations or self.config.max_iterations
        
        for iteration in range(iterations):
            # å„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®è©•ä¾¡
            for agent in self.agents:
                fitness = fitness_function(agent.position, agent.persona)
                
                # å€‹ä½“ãƒ™ã‚¹ãƒˆæ›´æ–°
                if fitness > agent.best_fitness:
                    agent.best_fitness = fitness
                    agent.best_position = agent.position.copy()
                
                # ç¾¤ãƒ™ã‚¹ãƒˆæ›´æ–°
                if fitness > self.global_best_fitness:
                    self.global_best_fitness = fitness
                    self.global_best_position = agent.position.copy()
            
            # é€Ÿåº¦ã¨ä½ç½®ã®æ›´æ–°
            for agent in self.agents:
                agent.update_velocity(
                    self.global_best_position,
                    self.config.inertia_weight,
                    self.config.cognitive_weight,
                    self.config.social_weight
                )
                agent.update_position()
            
            logger.debug(f"ğŸŒŠ Swarm iter {iteration}: Best fitness = {self.global_best_fitness:.4f}")
        
        return self.global_best_position, self.global_best_fitness
    
    def get_consensus(self) -> Dict[str, Any]:
        """ç¾¤ã®ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹ã‚’å–å¾—"""
        if not self.agents:
            return {}
        
        # å„ãƒšãƒ«ã‚½ãƒŠã‹ã‚‰ã®æ„è¦‹ã‚’é›†ç´„
        persona_positions = defaultdict(list)
        for agent in self.agents:
            persona_positions[agent.persona].append(agent.best_position)
        
        consensus = {}
        for persona, positions in persona_positions.items():
            consensus[persona.value] = {
                'mean_position': np.mean(positions, axis=0),
                'std': np.std(positions, axis=0),
                'confidence': np.mean([a.best_fitness for a in self.agents if a.persona == persona])
            }
        
        return consensus


# ==================== RLHF ====================

class RLHFTrainer:
    """Reinforcement Learning from Human Feedback"""
    
    def __init__(self, config: RLHFConfig):
        self.config = config
        self.q_table: Dict[Tuple[str, str], float] = defaultdict(float)  # (state, action) -> Qå€¤
        self.state_visits: Dict[str, int] = defaultdict(int)
        self.reward_history: List[float] = []
    
    def get_state(self, intent: Intent, complexity: Complexity) -> str:
        """çŠ¶æ…‹ã‚’å–å¾—"""
        return f"{intent.value}_{complexity.value}"
    
    def select_action(self, state: str, available_actions: List[str]) -> str:
        """è¡Œå‹•é¸æŠï¼ˆÎµ-greedyï¼‰"""
        if np.random.random() < self.config.exploration_rate:
            # æ¢ç´¢
            return np.random.choice(available_actions)
        else:
            # æ´»ç”¨
            q_values = [(action, self.q_table[(state, action)]) for action in available_actions]
            return max(q_values, key=lambda x: x[1])[0]
    
    def update(self, state: str, action: str, reward: float, next_state: str):
        """Qå€¤æ›´æ–°ï¼ˆQ-Learningï¼‰"""
        current_q = self.q_table[(state, action)]
        
        # æ¬¡çŠ¶æ…‹ã®æœ€å¤§Qå€¤
        next_q_values = [self.q_table[(next_state, a)] for a in [action]]  # ç°¡æ˜“ç‰ˆ
        max_next_q = max(next_q_values) if next_q_values else 0
        
        # Qå€¤æ›´æ–°
        new_q = current_q + self.config.learning_rate * (
            reward + self.config.discount_factor * max_next_q - current_q
        )
        
        self.q_table[(state, action)] = new_q
        self.state_visits[state] += 1
        self.reward_history.append(reward)
        
        logger.debug(f"ğŸ¯ RLHF: state={state}, action={action}, reward={reward:.3f}, Q={new_q:.3f}")
    
    def get_policy(self) -> Dict[str, str]:
        """ç¾åœ¨ã®ãƒãƒªã‚·ãƒ¼ã‚’å–å¾—"""
        policy = {}
        states = set(s for s, a in self.q_table.keys())
        
        for state in states:
            state_actions = [(a, q) for (s, a), q in self.q_table.items() if s == state]
            if state_actions:
                best_action = max(state_actions, key=lambda x: x[1])[0]
                policy[state] = best_action
        
        return policy


# ==================== ä»®èª¬æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ  ====================

class HypothesisTester:
    """è‡ªå‹•ä»®èª¬ç”Ÿæˆãƒ»æ¤œè¨¼ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self):
        self.hypotheses: List[Hypothesis] = []
        self.tested_count = 0
    
    def generate_hypothesis(self, context: str, observation: str) -> Hypothesis:
        """è¦³å¯Ÿã‹ã‚‰ä»®èª¬ã‚’ç”Ÿæˆ"""
        hypothesis_id = str(uuid.uuid4())[:8]
        
        # ç°¡æ˜“çš„ãªä»®èª¬ç”Ÿæˆï¼ˆå®Ÿéš›ã¯LLMã‚’ä½¿ç”¨ï¼‰
        statement = f"Based on '{observation}', the underlying principle might be related to {context}"
        
        hypothesis = Hypothesis(
            id=hypothesis_id,
            statement=statement,
            confidence=0.5,
            bayesian_prior=0.5
        )
        
        self.hypotheses.append(hypothesis)
        return hypothesis
    
    def test_hypothesis(self, hypothesis: Hypothesis, evidence: str, supports: bool):
        """ä»®èª¬ã‚’æ¤œè¨¼"""
        hypothesis.tested = True
        hypothesis.result = supports
        
        if supports:
            hypothesis.evidence.append(evidence)
        else:
            hypothesis.counter_evidence.append(evidence)
        
        # ãƒ™ã‚¤ã‚ºæ›´æ–°
        likelihood = 0.8 if supports else 0.2
        prior = hypothesis.bayesian_prior
        
        # ç°¡æ˜“çš„ãªãƒ™ã‚¤ã‚ºæ›´æ–°
        posterior = (likelihood * prior) / ((likelihood * prior) + ((1 - likelihood) * (1 - prior)))
        hypothesis.bayesian_posterior = posterior
        hypothesis.confidence = posterior
        
        self.tested_count += 1
        logger.info(f"ğŸ”¬ Hypothesis tested: {hypothesis.statement[:50]}... â†’ {supports} (conf: {posterior:.3f})")
    
    def get_best_hypotheses(self, top_k: int = 3) -> List[Hypothesis]:
        """æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹ä»®èª¬ã‚’å–å¾—"""
        tested = [h for h in self.hypotheses if h.tested]
        return sorted(tested, key=lambda h: h.confidence, reverse=True)[:top_k]


# ==================== çŸ¥è­˜ã‚°ãƒ©ãƒ• ====================

class AdvancedKnowledgeGraph:
    """é«˜åº¦ãªçŸ¥è­˜ã‚°ãƒ©ãƒ•"""
    
    def __init__(self):
        self.nodes: Dict[str, KnowledgeNode] = {}
        self.edges: List[KnowledgeEdge] = []
        self.communities: Dict[str, Set[str]] = {}  # ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡º
    
    def add_node(self, node: KnowledgeNode):
        """ãƒãƒ¼ãƒ‰è¿½åŠ """
        node.updated = datetime.now()
        if node.id in self.nodes:
            node.access_count = self.nodes[node.id].access_count + 1
        self.nodes[node.id] = node
    
    def add_edge(self, edge: KnowledgeEdge):
        """ã‚¨ãƒƒã‚¸è¿½åŠ """
        self.edges.append(edge)
    
    def get_neighbors(self, node_id: str, relation: Optional[str] = None) -> List[str]:
        """éš£æ¥ãƒãƒ¼ãƒ‰å–å¾—"""
        neighbors = []
        for edge in self.edges:
            if edge.source == node_id and (relation is None or edge.relation == relation):
                neighbors.append(edge.target)
            elif edge.target == node_id and (relation is None or edge.relation == relation):
                neighbors.append(edge.source)
        return neighbors
    
    def find_communities(self) -> Dict[str, Set[str]]:
        """ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£æ¤œå‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not self.nodes:
            return {}
        
        # é€£çµæˆåˆ†ã®æ¤œå‡º
        visited = set()
        communities = {}
        community_id = 0
        
        def dfs(node_id: str, community: Set[str]):
            visited.add(node_id)
            community.add(node_id)
            for neighbor in self.get_neighbors(node_id):
                if neighbor not in visited:
                    dfs(neighbor, community)
        
        for node_id in self.nodes:
            if node_id not in visited:
                community = set()
                dfs(node_id, community)
                communities[f"community_{community_id}"] = community
                community_id += 1
        
        self.communities = communities
        return communities
    
    def get_central_nodes(self, top_k: int = 5) -> List[Tuple[str, float]]:
        """ä¸­å¿ƒæ€§ã®é«˜ã„ãƒãƒ¼ãƒ‰å–å¾—"""
        # æ¬¡æ•°ä¸­å¿ƒæ€§
        degree_centrality = {}
        for node_id in self.nodes:
            degree = len(self.get_neighbors(node_id))
            degree_centrality[node_id] = degree
        
        sorted_nodes = sorted(degree_centrality.items(), key=lambda x: x[1], reverse=True)
        return sorted_nodes[:top_k]
    
    def query_subgraph(self, query: str, depth: int = 2) -> Dict[str, Any]:
        """ã‚¯ã‚¨ãƒªã«é–¢é€£ã™ã‚‹ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’å–å¾—"""
        # ã‚¯ã‚¨ãƒªã‹ã‚‰ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£ã‚’æŠ½å‡º
        query_words = set(re.findall(r'\b\w+\b', query.lower()))
        
        # é–¢é€£ãƒãƒ¼ãƒ‰ã‚’æ¤œç´¢
        relevant_nodes = []
        for node_id, node in self.nodes.items():
            node_words = set(re.findall(r'\b\w+\b', node.name.lower()))
            overlap = len(query_words & node_words)
            if overlap > 0:
                node.relevance_score = overlap / len(query_words)
                relevant_nodes.append(node_id)
        
        if not relevant_nodes:
            return {'nodes': [], 'edges': []}
        
        # æ·±ã•å„ªå…ˆã§ã‚µãƒ–ã‚°ãƒ©ãƒ•ã‚’å±•é–‹
        subgraph_nodes = set(relevant_nodes)
        for _ in range(depth):
            new_nodes = set()
            for node_id in list(subgraph_nodes):
                new_nodes.update(self.get_neighbors(node_id))
            subgraph_nodes.update(new_nodes)
        
        subgraph_edges = [
            e for e in self.edges
            if e.source in subgraph_nodes and e.target in subgraph_nodes
        ]
        
        return {
            'nodes': [self.nodes[nid] for nid in subgraph_nodes],
            'edges': subgraph_edges,
            'central_node': relevant_nodes[0] if relevant_nodes else None
        }


# ==================== ãƒ¡ã‚¤ãƒ³ã‚·ã‚¹ãƒ†ãƒ  ====================

class QuantumLLM:
    """Quantum-Enhanced LLM System"""
    
    MODELS = {
        'llama-3.1-8b-instant': {'cost': 'low', 'quality': 'medium', 'speed': 'fast'},
        'llama-3.1-70b-versatile': {'cost': 'medium', 'quality': 'high', 'speed': 'medium'},
        'llama-3.3-70b-versatile': {'cost': 'medium', 'quality': 'high', 'speed': 'medium'},
    }
    
    def __init__(self, api_key: Optional[str] = None, config: Optional[SystemConfig] = None):
        self.api_key = api_key or os.environ.get('GROQ_API_KEY')
        if not self.api_key:
            raise ValueError("âŒ GROQ_API_KEY required")
        
        self.config = config or SystemConfig()
        self.client = Groq(api_key=self.api_key)
        
        # ã‚³ã‚¢ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.vector_db = VectorDB(self.config.vec_dim) if self.config.vec_db else None
        self.knowledge_graph = AdvancedKnowledgeGraph() if self.config.knowledge_graph else None
        
        # é«˜åº¦ãªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ
        self.quantum_optimizer = QuantumOptimizer(self.config.quantum) if self.config.quantum.enabled else None
        self.genetic_evolver = GeneticPromptEvolver(self.config.genetic) if self.config.genetic.enabled else None
        self.swarm = SwarmIntelligence(self.config.swarm) if self.config.swarm.enabled else None
        self.rlhf = RLHFTrainer(self.config.rlhf) if self.config.rlhf.enabled else None
        self.hypothesis_tester = HypothesisTester() if self.config.hypothesis_testing else None
        
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        self.profile = self._init_profile()
        
        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹
        self.metrics = {
            'queries': 0,
            'success': 0,
            'total_cost': 0,
            'total_tokens': 0,
            'cache_hits': 0,
            'quantum_optimizations': 0,
            'genetic_evolutions': 0,
            'swarm_optimizations': 0,
            'hypotheses_tested': 0
        }
        
        # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        self.context_window = deque(maxlen=20)
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé›†å›£ã®åˆæœŸåŒ–
        if self.genetic_evolver:
            base_prompts = [
                "Provide a clear and comprehensive answer.",
                "Think step by step and explain your reasoning.",
                "Analyze the question from multiple perspectives.",
            ]
            self.genetic_evolver.initialize_population(base_prompts, "general")
        
        logger.info(f"âœ… Quantum-Enhanced LLM initialized")
        self._log_features()
    
    def _init_profile(self) -> Dict[str, Any]:
        """ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«åˆæœŸåŒ–"""
        return {
            'topics': defaultdict(int),
            'expertise': defaultdict(float),
            'strategy_preference': defaultdict(float),
            'interaction_count': 0,
            'feedback_history': []
        }
    
    def _log_features(self):
        """æœ‰åŠ¹æ©Ÿèƒ½ã‚’ãƒ­ã‚°å‡ºåŠ›"""
        features = []
        if self.config.quantum.enabled:
            features.append("ğŸ”®Quantum")
        if self.config.genetic.enabled:
            features.append("ğŸ§¬Genetic")
        if self.config.swarm.enabled:
            features.append("ğŸŒŠSwarm")
        if self.config.rlhf.enabled:
            features.append("ğŸ¯RLHF")
        if self.config.hypothesis_testing:
            features.append("ğŸ”¬Hypothesis")
        if self.config.knowledge_graph:
            features.append("ğŸ§©KG")
        
        logger.info(" | ".join(features))
    
    def _analyze_query(self, query: str) -> Tuple[Intent, Complexity]:
        """ã‚¯ã‚¨ãƒªã‚’åˆ†æ"""
        q = query.lower()
        
        # æ„å›³åˆ†æ
        intent_patterns = {
            Intent.REASONING: ['why', 'because', 'reason', 'cause'],
            Intent.ANALYSIS: ['analyze', 'compare', 'evaluate', 'assess'],
            Intent.RESEARCH: ['research', 'investigate', 'study', 'explore'],
            Intent.PLANNING: ['plan', 'strategy', 'organize', 'schedule'],
            Intent.TECHNICAL: ['code', 'algorithm', 'implement', 'debug'],
            Intent.CREATIVE: ['create', 'write', 'design', 'imagine'],
            Intent.DEBUGGING: ['bug', 'error', 'fix', 'debug', 'issue'],
            Intent.OPTIMIZATION: ['optimize', 'improve', 'enhance', 'better']
        }
        
        intent = Intent.QUESTION
        max_matches = 0
        for int_type, patterns in intent_patterns.items():
            matches = sum(1 for p in patterns if p in q)
            if matches > max_matches:
                max_matches = matches
                intent = int_type
        
        # è¤‡é›‘åº¦åˆ†æ
        complexity_score = 0
        complexity_score += len(query) // 100
        complexity_score += q.count('?')
        
        frontier_words = ['breakthrough', 'novel', 'unprecedented', 'cutting-edge']
        research_words = ['hypothesis', 'theory', 'prove', 'demonstrate']
        expert_words = ['advanced', 'sophisticated', 'complex', 'intricate']
        
        complexity_score += sum(5 for w in frontier_words if w in q)
        complexity_score += sum(4 for w in research_words if w in q)
        complexity_score += sum(3 for w in expert_words if w in q)
        
        if complexity_score < 2:
            complexity = Complexity.TRIVIAL
        elif complexity_score < 4:
            complexity = Complexity.SIMPLE
        elif complexity_score < 7:
            complexity = Complexity.MEDIUM
        elif complexity_score < 11:
            complexity = Complexity.COMPLEX
        elif complexity_score < 16:
            complexity = Complexity.EXPERT
        elif complexity_score < 20:
            complexity = Complexity.RESEARCH
        else:
            complexity = Complexity.FRONTIER
        
        return intent, complexity
    
    def _select_strategy(self, intent: Intent, complexity: Complexity) -> Strategy:
        """æˆ¦ç•¥é¸æŠ"""
        # ãƒ•ãƒ­ãƒ³ãƒ†ã‚£ã‚¢ãƒ¬ãƒ™ãƒ«: é‡å­æœ€é©åŒ–
        if complexity == Complexity.FRONTIER and self.config.quantum.enabled:
            return Strategy.QUANTUM
        
        # ç ”ç©¶ãƒ¬ãƒ™ãƒ«: éºä¼çš„é€²åŒ–
        if complexity == Complexity.RESEARCH and self.config.genetic.enabled:
            return Strategy.GENETIC
        
        # è¤‡é›‘ãªæ¨è«–: ç¾¤çŸ¥èƒ½
        if complexity in [Complexity.EXPERT, Complexity.COMPLEX] and self.config.swarm.enabled:
            return Strategy.SWARM
        
        # åˆ†æãƒ»æ¨è«–: Tree of Thoughts
        if intent in [Intent.ANALYSIS, Intent.REASONING] and self.config.tree_of_thoughts:
            return Strategy.TREE_SEARCH
        
        # è¨è«–ãŒæœ‰åŠ¹ãªå ´åˆ
        if complexity in [Complexity.EXPERT, Complexity.RESEARCH] and self.config.debate_mode:
            return Strategy.DEBATE
        
        # Chain of Thought
        if complexity >= Complexity.COMPLEX and self.config.chain_of_thought:
            return Strategy.COT
        
        # RLHFæ¨å¥¨ãŒã‚ã‚‹å ´åˆ
        if self.rlhf:
            state = self.rlhf.get_state(intent, complexity)
            available_strategies = [s.value for s in Strategy]
            recommended = self.rlhf.select_action(state, available_strategies)
            try:
                return Strategy(recommended)
            except:
                pass
        
        return Strategy.DIRECT
    
    async def _call_api(
        self,
        model: str,
        messages: List[Dict],
        temperature: float,
        max_tokens: int
    ):
        """APIå‘¼ã³å‡ºã—"""
        for attempt in range(self.config.max_retries):
            try:
                return self.client.chat.completions.create(
                    model=model,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
            except (RateLimitError, APIError) as e:
                if attempt == self.config.max_retries - 1:
                    raise
                wait_time = self.config.retry_delay * (2 ** attempt)
                logger.warning(f"Retry {attempt + 1}/{self.config.max_retries}")
                await asyncio.sleep(wait_time)
    
    def _build_system_prompt(
        self,
        query: str,
        intent: Intent,
        complexity: Complexity,
        strategy: Strategy
    ) -> str:
        """ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ§‹ç¯‰"""
        base = "You are an advanced AI assistant with quantum-inspired reasoning capabilities."
        
        # æˆ¦ç•¥åˆ¥ã®æŒ‡ç¤º
        strategy_instructions = {
            Strategy.QUANTUM: "Use multi-dimensional thinking. Explore superposition of possibilities.",
            Strategy.GENETIC: "Evolve your answer through iterative refinement.",
            Strategy.SWARM: "Consider diverse perspectives and find consensus.",
            Strategy.COT: "Think step by step. Show your reasoning process.",
            Strategy.DEBATE: "Present multiple viewpoints and synthesize them.",
            Strategy.TREE_SEARCH: "Explore different reasoning paths systematically."
        }
        
        strategy_text = strategy_instructions.get(strategy, "")
        
        # è¤‡é›‘åº¦åˆ¥ã®èª¿æ•´
        if complexity in [Complexity.RESEARCH, Complexity.FRONTIER]:
            complexity_text = "Provide research-grade analysis with novel insights."
        elif complexity == Complexity.EXPERT:
            complexity_text = "Provide expert-level insights with technical depth."
        else:
            complexity_text = "Provide clear, well-structured answers."
        
        # çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‹ã‚‰ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        kg_context = ""
        if self.knowledge_graph:
            subgraph = self.knowledge_graph.query_subgraph(query, depth=1)
            if subgraph['nodes']:
                node_names = [n.name for n in subgraph['nodes'][:3]]
                kg_context = f" Related concepts: {', '.join(node_names)}."
        
        prompt = f"{base} {strategy_text} {complexity_text}{kg_context}"
        
        return prompt.strip()
    
    async def _execute_quantum_strategy(
        self,
        query: str,
        model: str,
        intent: Intent,
        complexity: Complexity
    ) -> Response:
        """é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢æˆ¦ç•¥"""
        logger.info("ğŸ”® Executing quantum-inspired optimization")
        self.metrics['quantum_optimizations'] += 1
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç©ºé–“ã‚’é‡å­æœ€é©åŒ–
        def objective(params):
            temp, top_p, freq_penalty = params[0], params[1], params[2]
            # ç°¡æ˜“è©•ä¾¡é–¢æ•°ï¼ˆå®Ÿéš›ã¯å¿œç­”å“è³ªã§è©•ä¾¡ï¼‰
            score = 1.0 - abs(temp - 0.7) - abs(top_p - 0.9) - abs(freq_penalty - 0.1)
            return score
        
        optimized_params, _ = self.quantum_optimizer.optimize_parameters(objective)
        
        temperature = float(optimized_params[0])
        system_prompt = self._build_system_prompt(query, intent, complexity, Strategy.QUANTUM)
        
        start_time = time.time()
        api_response = await self._call_api(
            model,
            [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            temperature,
            self.config.max_tokens
        )
        latency = (time.time() - start_time) * 1000
        
        response = self._build_response(api_response, model, Strategy.QUANTUM, latency)
        response.quantum_optimized = True
        
        return response
    
    async def _execute_genetic_strategy(
        self,
        query: str,
        model: str,
        intent: Intent,
        complexity: Complexity
    ) -> Response:
        """éºä¼çš„é€²åŒ–æˆ¦ç•¥"""
        logger.info("ğŸ§¬ Executing genetic evolution")
        self.metrics['genetic_evolutions'] += 1
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’é€²åŒ–ã•ã›ã‚‹
        def fitness_func(prompt: Prompt):
            # ç°¡æ˜“è©•ä¾¡ï¼ˆå®Ÿéš›ã¯å¿œç­”å“è³ªã§è©•ä¾¡ï¼‰
            return prompt.success_rate * 0.5 + prompt.avg_quality * 0.5
        
        for _ in range(3):  # 3ä¸–ä»£é€²åŒ–
            best_prompt = self.genetic_evolver.evolve(fitness_func)
        
        system_prompt = self._build_system_prompt(query, intent, complexity, Strategy.GENETIC)
        enhanced_query = f"{best_prompt.template}\n\n{query}"
        
        start_time = time.time()
        api_response = await self._call_api(
            model,
            [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": enhanced_query}
            ],
            0.7,
            self.config.max_tokens
        )
        latency = (time.time() - start_time) * 1000
        
        response = self._build_response(api_response, model, Strategy.GENETIC, latency)
        response.genetic_fitness = best_prompt.fitness
        
        return response
    
    async def _execute_swarm_strategy(
        self,
        query: str,
        model: str,
        intent: Intent,
        complexity: Complexity
    ) -> Response:
        """ç¾¤çŸ¥èƒ½æˆ¦ç•¥"""
        logger.info("ğŸŒŠ Executing swarm intelligence")
        self.metrics['swarm_optimizations'] += 1
        
        # å„ãƒšãƒ«ã‚½ãƒŠã‹ã‚‰ã®å¿œç­”ã‚’åé›†
        personas = [PersonaType.OPTIMIST, PersonaType.PESSIMIST, PersonaType.PRAGMATIST]
        responses = []
        
        for persona in personas:
            persona_prompt = f"As a {persona.value}, answer: {query}"
            system_prompt = self._build_system_prompt(query, intent, complexity, Strategy.SWARM)
            
            try:
                api_response = await self._call_api(
                    model,
                    [
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": persona_prompt}
                    ],
                    0.7,
                    self.config.max_tokens // 2
                )
                
                text = api_response.choices[0].message.content or ""
                responses.append({
                    'persona': persona.value,
                    'text': text,
                    'confidence': 0.7 + np.random.random() * 0.2
                })
            except Exception as e:
                logger.warning(f"Swarm agent {persona.value} failed: {e}")
        
        if not responses:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            return await self._execute_direct(query, model, intent, complexity)
        
        # ã‚³ãƒ³ã‚»ãƒ³ã‚µã‚¹åˆæˆ
        synthesis_prompt = f"Synthesize these perspectives:\n\n"
        for resp in responses:
            synthesis_prompt += f"{resp['persona']}: {resp['text'][:200]}...\n\n"
        synthesis_prompt += f"\nProvide a balanced synthesis answering: {query}"
        
        start_time = time.time()
        final_response = await self._call_api(
            model,
            [
                {"role": "system", "content": "Synthesize multiple perspectives into a coherent answer."},
                {"role": "user", "content": synthesis_prompt}
            ],
            0.7,
            self.config.max_tokens
        )
        latency = (time.time() - start_time) * 1000
        
        response = self._build_response(final_response, model, Strategy.SWARM, latency)
        response.personas_involved = [r['persona'] for r in responses]
        response.swarm_consensus = statistics.mean(r['confidence'] for r in responses)
        response.alternatives = [{'persona': r['persona'], 'text': r['text'][:100]} for r in responses]
        
        return response
    
    async def _execute_direct(
        self,
        query: str,
        model: str,
        intent: Intent,
        complexity: Complexity
    ) -> Response:
        """ç›´æ¥å®Ÿè¡Œ"""
        system_prompt = self._build_system_prompt(query, intent, complexity, Strategy.DIRECT)
        
        start_time = time.time()
        api_response = await self._call_api(
            model,
            [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": query}
            ],
            0.7,
            self.config.max_tokens
        )
        latency = (time.time() - start_time) * 1000
        
        return self._build_response(api_response, model, Strategy.DIRECT, latency)
    
    def _build_response(
        self,
        api_response,
        model: str,
        strategy: Strategy,
        latency: float
    ) -> Response:
        """å¿œç­”ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæ§‹ç¯‰"""
        choice = api_response.choices[0]
        text = choice.message.content or ""
        
        usage = api_response.usage
        cost = self._calculate_cost(model, usage.prompt_tokens, usage.completion_tokens)
        
        # å“è³ªã‚¹ã‚³ã‚¢è¨ˆç®—
        coherence = min(1.0, len(text.split('.')) / 10)
        relevance = 0.8
        completeness = min(1.0, len(text) / 500)
        factuality = 0.85
        novelty = 0.7 if strategy in [Strategy.QUANTUM, Strategy.GENETIC] else 0.5
        
        # ä¿¡é ¼åº¦è¨ˆç®—
        base_confidence = 0.9 if choice.finish_reason == "stop" else 0.75
        uncertainty = sum(0.1 for phrase in ['maybe', 'perhaps', 'possibly'] if phrase in text.lower())
        confidence = max(0, min(1, base_confidence - uncertainty * 0.1))
        
        return Response(
            text=text,
            confidence=confidence,
            tokens=usage.total_tokens,
            prompt_tokens=usage.prompt_tokens,
            completion_tokens=usage.completion_tokens,
            latency=latency,
            cost=cost,
            model=model,
            finish_reason=choice.finish_reason,
            strategy=strategy,
            uncertainty=min(1.0, uncertainty),
            coherence_score=coherence,
            relevance_score=relevance,
            completeness_score=completeness,
            factuality_score=factuality,
            novelty_score=novelty
        )
    
    def _calculate_cost(self, model: str, prompt_tokens: int, completion_tokens: int) -> float:
        """ã‚³ã‚¹ãƒˆè¨ˆç®—"""
        pricing = {
            'llama-3.1-8b-instant': {'input': 0.05 / 1e6, 'output': 0.08 / 1e6},
            'llama-3.1-70b-versatile': {'input': 0.59 / 1e6, 'output': 0.79 / 1e6},
            'llama-3.3-70b-versatile': {'input': 0.59 / 1e6, 'output': 0.79 / 1e6},
        }
        p = pricing.get(model, {'input': 0.0001 / 1e6, 'output': 0.0001 / 1e6})
        return prompt_tokens * p['input'] + completion_tokens * p['output']
    
    async def query_async(self, query: str, **kwargs) -> Response:
        """ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªå‡¦ç†ï¼ˆéåŒæœŸï¼‰"""
        self.metrics['queries'] += 1
        
        try:
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            if self.vector_db:
                cached_results = self.vector_db.search(query, top_k=1, min_similarity=self.config.similarity_threshold)
                if cached_results:
                    _, similarity, metadata = cached_results[0]
                    if time.time() - metadata.get('added_at', 0) < self.config.cache_ttl:
                        self.metrics['cache_hits'] += 1
                        logger.info(f"ğŸ”„ Cache hit: {similarity:.3f}")
                        resp_data = metadata.get('response', {})
                        return Response(
                            text=resp_data.get('text', ''),
                            confidence=resp_data.get('confidence', 0),
                            cached=True,
                            similarity=similarity,
                            **{k: v for k, v in resp_data.items() if k not in ['text', 'confidence']}
                        )
            
            # ã‚¯ã‚¨ãƒªåˆ†æ
            intent, complexity = self._analyze_query(query)
            strategy = self._select_strategy(intent, complexity)
            
            model = kwargs.get('model', self.config.model)
            
            # æˆ¦ç•¥å®Ÿè¡Œ
            if strategy == Strategy.QUANTUM and self.quantum_optimizer:
                response = await self._execute_quantum_strategy(query, model, intent, complexity)
            elif strategy == Strategy.GENETIC and self.genetic_evolver:
                response = await self._execute_genetic_strategy(query, model, intent, complexity)
            elif strategy == Strategy.SWARM and self.swarm:
                response = await self._execute_swarm_strategy(query, model, intent, complexity)
            else:
                response = await self._execute_direct(query, model, intent, complexity)
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨­å®š
            response.intent = intent
            response.complexity = complexity
            
            # ãƒ¡ãƒˆãƒªã‚¯ã‚¹æ›´æ–°
            if response.success:
                self.metrics['success'] += 1
            self.metrics['total_cost'] += response.cost
            self.metrics['total_tokens'] += response.tokens
            
            # RLHFæ›´æ–°
            if self.rlhf:
                state = self.rlhf.get_state(intent, complexity)
                reward = response.quality_score
                next_state = state  # ç°¡æ˜“ç‰ˆ
                self.rlhf.update(state, strategy.value, reward, next_state)
            
            # ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ›´æ–°
            self.context_window.append(query[:100])
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
            if self.vector_db and response.success:
                self.vector_db.add(
                    str(uuid.uuid4())[:8],
                    query,
                    {'response': response.to_dict()}
                )
            
            # çŸ¥è­˜ã‚°ãƒ©ãƒ•æ›´æ–°
            if self.knowledge_graph:
                self._update_knowledge_graph(query, response.text)
            
            return response
        
        except Exception as e:
            logger.error(f"Query failed: {e}")
            return Response(
                text=f"âŒ Error: {str(e)}",
                confidence=0,
                finish_reason="error"
            )
    
    def query(self, query: str, **kwargs) -> Response:
        """ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªå‡¦ç†ï¼ˆåŒæœŸï¼‰"""
        return asyncio.run(self.query_async(query, **kwargs))
    
    def _update_knowledge_graph(self, query: str, response: str):
        """çŸ¥è­˜ã‚°ãƒ©ãƒ•ã‚’æ›´æ–°"""
        # ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰
        entities = re.findall(r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b', response)
        
        for entity in set(entities[:5]):
            node_id = hashlib.md5(entity.encode()).hexdigest()[:8]
            node = KnowledgeNode(
                id=node_id,
                name=entity,
                type='entity',
                properties={'source': 'response'}
            )
            self.knowledge_graph.add_node(node)
        
        # é–¢ä¿‚æŠ½å‡ºï¼ˆéš£æ¥ã‚¨ãƒ³ãƒ†ã‚£ãƒ†ã‚£é–“ï¼‰
        for i in range(len(entities) - 1):
            source_id = hashlib.md5(entities[i].encode()).hexdigest()[:8]
            target_id = hashlib.md5(entities[i + 1].encode()).hexdigest()[:8]
            
            if source_id in self.knowledge_graph.nodes and target_id in self.knowledge_graph.nodes:
                edge = KnowledgeEdge(
                    source=source_id,
                    target=target_id,
                    relation='mentioned_with',
                    weight=0.5
                )
                self.knowledge_graph.add_edge(edge)
    
    def add_feedback(self, query: str, response: str, rating: int, response_obj: Optional[Response] = None):
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯è¿½åŠ """
        self.profile['interaction_count'] += 1
        self.profile['feedback_history'].append({
            'query': query[:100],
            'response': response[:100],
            'rating': rating,
            'timestamp': datetime.now().isoformat()
        })
        
        # ãƒˆãƒ”ãƒƒã‚¯æ›´æ–°
        words = re.findall(r'\b\w{4,}\b', query.lower())
        for word in words:
            self.profile['topics'][word] += rating
            if rating > 0:
                self.profile['expertise'][word] = min(1.0, self.profile['expertise'][word] + 0.1)
        
        # æˆ¦ç•¥å¥½ã¿æ›´æ–°
        if response_obj and response_obj.strategy:
            current = self.profile['strategy_preference'][response_obj.strategy.value]
            self.profile['strategy_preference'][response_obj.strategy.value] = current + rating * 0.1
        
        # éºä¼çš„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ›´æ–°
        if self.genetic_evolver and response_obj:
            for prompt in self.genetic_evolver.population:
                if prompt.usage_count > 0:
                    if rating > 0:
                        prompt.success_count += 1
                    prompt.avg_quality = (prompt.avg_quality * (prompt.usage_count - 1) + abs(rating)) / prompt.usage_count
                    prompt.fitness = prompt.success_rate * 0.5 + prompt.avg_quality * 0.5
        
        logger.info(f"ğŸ¯ Feedback: {rating:+d} | Strategy: {response_obj.strategy if response_obj else 'N/A'}")
    
    def get_stats(self) -> Dict:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        stats = {
            'system': {
                'queries': self.metrics['queries'],
                'success_rate': f"{self.metrics['success'] / max(self.metrics['queries'], 1):.1%}",
                'cache_hit_rate': f"{self.metrics['cache_hits'] / max(self.metrics['queries'], 1):.1%}",
                'total_cost': f"${self.metrics['total_cost']:.6f}",
                'avg_cost': f"${self.metrics['total_cost'] / max(self.metrics['queries'], 1):.6f}"
            },
            'advanced': {
                'quantum_optimizations': self.metrics['quantum_optimizations'],
                'genetic_evolutions': self.metrics['genetic_evolutions'],
                'swarm_optimizations': self.metrics['swarm_optimizations'],
                'hypotheses_tested': self.metrics['hypotheses_tested']
            },
            'profile': {
                'interactions': self.profile['interaction_count'],
                'top_topics': sorted(self.profile['topics'].items(), key=lambda x: x[1], reverse=True)[:5],
                'expertise_areas': len([e for e in self.profile['expertise'].values() if e > 0.5])
            }
        }
        
        # çŸ¥è­˜ã‚°ãƒ©ãƒ•çµ±è¨ˆ
        if self.knowledge_graph:
            stats['knowledge_graph'] = {
                'nodes': len(self.knowledge_graph.nodes),
                'edges': len(self.knowledge_graph.edges),
                'communities': len(self.knowledge_graph.communities)
            }
        
        # éºä¼çš„é€²åŒ–çµ±è¨ˆ
        if self.genetic_evolver:
            best_prompts = self.genetic_evolver.get_best_prompts(3)
            stats['genetic'] = {
                'generation': self.genetic_evolver.generation,
                'population_size': len(self.genetic_evolver.population),
                'best_fitness': best_prompts[0].fitness if best_prompts else 0
            }
        
        # RLHFçµ±è¨ˆ
        if self.rlhf:
            stats['rlhf'] = {
                'states_explored': len(self.rlhf.state_visits),
                'total_updates': sum(self.rlhf.state_visits.values()),
                'avg_reward': statistics.mean(self.rlhf.reward_history) if self.rlhf.reward_history else 0
            }
        
        return stats
    
    def save_state(self, filepath: str = 'quantum_llm_state.json'):
        """çŠ¶æ…‹ä¿å­˜"""
        try:
            state = {
                'profile': {
                    'topics': dict(self.profile['topics']),
                    'expertise': dict(self.profile['expertise']),
                    'strategy_preference': dict(self.profile['strategy_preference']),
                    'interaction_count': self.profile['interaction_count'],
                    'feedback_history': self.profile['feedback_history']
                },
                'metrics': self.metrics,
                'timestamp': datetime.now().isoformat()
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(state, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ State saved: {filepath}")
        except Exception as e:
            logger.error(f"âŒ Save failed: {e}")
    
    def load_state(self, filepath: str = 'quantum_llm_state.json'):
        """çŠ¶æ…‹èª­ã¿è¾¼ã¿"""
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                state = json.load(f)
            
            profile_data = state.get('profile', {})
            self.profile['topics'] = defaultdict(int, profile_data.get('topics', {}))
            self.profile['expertise'] = defaultdict(float, profile_data.get('expertise', {}))
            self.profile['strategy_preference'] = defaultdict(float, profile_data.get('strategy_preference', {}))
            self.profile['interaction_count'] = profile_data.get('interaction_count', 0)
            self.profile['feedback_history'] = profile_data.get('feedback_history', [])
            
            self.metrics.update(state.get('metrics', {}))
            
            logger.info(f"ğŸ“‚ State loaded: {filepath}")
        except FileNotFoundError:
            logger.info("â„¹ï¸  No saved state found")
        except Exception as e:
            logger.error(f"âŒ Load failed: {e}")


# ==================== ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒãƒ£ãƒƒãƒˆ ====================

class QuantumChat:
    """é‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒãƒ£ãƒƒãƒˆã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹"""
    
    def __init__(self, llm: QuantumLLM):
        self.llm = llm
        self.history: List[Tuple[str, Response]] = []
        self.session_id = str(uuid.uuid4())[:8]
    
    def print_welcome(self):
        """ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
        print("\n" + "=" * 80)
        print("ğŸ”® Quantum-Enhanced Self-Evolving LLM System v3.0Î³")
        print("=" * 80)
        print("\nâœ¨ é©æ–°çš„æ©Ÿèƒ½:")
        print("  ğŸ”® Quantum-Inspired Optimization")
        print("  ğŸ§¬ Genetic Algorithm for Prompt Evolution")
        print("  ğŸŒŠ Swarm Intelligence Multi-Agent System")
        print("  ğŸ¯ Reinforcement Learning from Human Feedback")
        print("  ğŸ”¬ Automated Hypothesis Testing")
        print("  ğŸ§© Advanced Knowledge Graph")
        print("\nã‚³ãƒãƒ³ãƒ‰:")
        print("  /stats      - çµ±è¨ˆæƒ…å ±")
        print("  /save       - çŠ¶æ…‹ä¿å­˜")
        print("  /load       - çŠ¶æ…‹èª­ã¿è¾¼ã¿")
        print("  /feedback <rating> - è©•ä¾¡ (-2 to +2)")
        print("  /quantum    - é‡å­æœ€é©åŒ–æƒ…å ±")
        print("  /genetic    - éºä¼çš„é€²åŒ–æƒ…å ±")
        print("  /swarm      - ç¾¤çŸ¥èƒ½æƒ…å ±")
        print("  /kg         - çŸ¥è­˜ã‚°ãƒ©ãƒ•æƒ…å ±")
        print("  /help       - ãƒ˜ãƒ«ãƒ—")
        print("  /exit       - çµ‚äº†")
        print("=" * 80 + "\n")
    
    def print_response(self, response: Response):
        """å¿œç­”è¡¨ç¤º"""
        print(f"\nğŸ¤– Assistant [{response.model.split('-')[-1]}]:")
        print("â”€" * 80)
        print(response.text)
        print("â”€" * 80)
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        metadata = []
        
        if response.strategy:
            emoji = {
                Strategy.QUANTUM: "ğŸ”®",
                Strategy.GENETIC: "ğŸ§¬",
                Strategy.SWARM: "ğŸŒŠ",
                Strategy.TREE_SEARCH: "ğŸŒ³",
                Strategy.COT: "ğŸ¤”",
                Strategy.DEBATE: "ğŸ—£ï¸"
            }.get(response.strategy, "ğŸ“‹")
            metadata.append(f"{emoji}{response.strategy.value}")
        
        if response.complexity:
            metadata.append(f"âš™ï¸{response.complexity.value}")
        
        metadata.append(f"â­{response.quality_score:.2f}")
        metadata.append(f"âœ…{response.confidence:.2f}")
        metadata.append(f"ğŸ²{response.uncertainty:.2f}")
        metadata.append(f"ğŸ’°${response.cost:.6f}")
        metadata.append(f"â±ï¸{response.latency:.0f}ms")
        
        if response.quantum_optimized:
            metadata.append("ğŸ”®Optimized")
        if response.genetic_fitness > 0:
            metadata.append(f"ğŸ§¬Fit:{response.genetic_fitness:.2f}")
        if response.swarm_consensus > 0:
            metadata.append(f"ğŸŒŠConsensus:{response.swarm_consensus:.2f}")
        if response.cached:
            metadata.append(f"ğŸ’¾Cache")
        
        print(" | ".join(metadata))
        
        # è¿½åŠ æƒ…å ±
        if response.personas_involved:
            print(f"\nğŸ­ Personas: {', '.join(response.personas_involved)}")
        
        if response.reasoning_steps:
            print(f"\nğŸ§  Reasoning Steps: {len(response.reasoning_steps)} steps")
        
        if response.alternatives:
            print(f"\nğŸ”„ Alternatives: {len(response.alternatives)} considered")
        
        print()
    
    def print_stats(self):
        """çµ±è¨ˆè¡¨ç¤º"""
        stats = self.llm.get_stats()
        
        print("\n" + "=" * 80)
        print("ğŸ“Š System Statistics")
        print("=" * 80)
        
        # ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆ
        sys = stats['system']
        print(f"\nğŸ“ˆ System:")
        print(f"   Queries: {sys['queries']} | Success Rate: {sys['success_rate']}")
        print(f"   Cache Hit Rate: {sys['cache_hit_rate']}")
        print(f"   Total Cost: {sys['total_cost']} | Avg: {sys['avg_cost']}")
        
        # é«˜åº¦ãªæ©Ÿèƒ½
        adv = stats['advanced']
        print(f"\nğŸš€ Advanced Features:")
        print(f"   ğŸ”® Quantum Optimizations: {adv['quantum_optimizations']}")
        print(f"   ğŸ§¬ Genetic Evolutions: {adv['genetic_evolutions']}")
        print(f"   ğŸŒŠ Swarm Optimizations: {adv['swarm_optimizations']}")
        print(f"   ğŸ”¬ Hypotheses Tested: {adv['hypotheses_tested']}")
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ«
        prof = stats['profile']
        print(f"\nğŸ‘¤ Profile:")
        print(f"   Interactions: {prof['interactions']}")
        print(f"   Expertise Areas: {prof['expertise_areas']}")
        if prof['top_topics']:
            print(f"   Top Topics: {', '.join([t[0] for t in prof['top_topics'][:3]])}")
        
        # çŸ¥è­˜ã‚°ãƒ©ãƒ•
        if 'knowledge_graph' in stats:
            kg = stats['knowledge_graph']
            print(f"\nğŸ§© Knowledge Graph:")
            print(f"   Nodes: {kg['nodes']} | Edges: {kg['edges']} | Communities: {kg['communities']}")
        
        # éºä¼çš„é€²åŒ–
        if 'genetic' in stats:
            gen = stats['genetic']
            print(f"\nğŸ§¬ Genetic Evolution:")
            print(f"   Generation: {gen['generation']} | Population: {gen['population_size']}")
            print(f"   Best Fitness: {gen['best_fitness']:.3f}")
        
        # RLHF
        if 'rlhf' in stats:
            rl = stats['rlhf']
            print(f"\nğŸ¯ RLHF:")
            print(f"   States Explored: {rl['states_explored']}")
            print(f"   Total Updates: {rl['total_updates']}")
            print(f"   Avg Reward: {rl['avg_reward']:.3f}")
        
        print("=" * 80 + "\n")
    
    def handle_command(self, command: str) -> bool:
        """ã‚³ãƒãƒ³ãƒ‰å‡¦ç†"""
        parts = command.strip().split()
        cmd = parts[0].lower()
        
        if cmd == '/exit':
            print("ğŸ‘‹ Goodbye!")
            return False
        
        elif cmd == '/stats':
            self.print_stats()
        
        elif cmd == '/save':
            filepath = parts[1] if len(parts) > 1 else 'quantum_llm_state.json'
            self.llm.save_state(filepath)
        
        elif cmd == '/load':
            filepath = parts[1] if len(parts) > 1 else 'quantum_llm_state.json'
            self.llm.load_state(filepath)
        
        elif cmd == '/feedback':
            if not self.history:
                print("âŒ No previous response to rate")
                return True
            
            try:
                rating = int(parts[1]) if len(parts) > 1 else 0
                if rating < -2 or rating > 2:
                    print("âŒ Rating must be between -2 and +2")
                    return True
                
                last_query, last_response = self.history[-1]
                self.llm.add_feedback(last_query, last_response.text, rating, last_response)
                print(f"âœ… Feedback recorded: {rating:+d}")
            
            except ValueError:
                print("âŒ Invalid rating")
        
        elif cmd == '/quantum':
            if self.llm.quantum_optimizer:
                print("\nğŸ”® Quantum Optimization Status:")
                print(f"   Enabled: Yes")
                print(f"   Qubits: {self.llm.quantum_optimizer.num_qubits}")
                print(f"   Iterations: {self.llm.quantum_optimizer.config.iterations}")
                print(f"   Total Optimizations: {self.llm.metrics['quantum_optimizations']}")
            else:
                print("âŒ Quantum optimization disabled")
        
        elif cmd == '/genetic':
            if self.llm.genetic_evolver:
                print("\nğŸ§¬ Genetic Evolution Status:")
                print(f"   Generation: {self.llm.genetic_evolver.generation}")
                print(f"   Population: {len(self.llm.genetic_evolver.population)}")
                best = self.llm.genetic_evolver.get_best_prompts(3)
                if best:
                    print(f"\n   Top 3 Prompts:")
                    for i, prompt in enumerate(best, 1):
                        print(f"   {i}. Fitness: {prompt.fitness:.3f} | {prompt.template[:50]}...")
            else:
                print("âŒ Genetic evolution disabled")
        
        elif cmd == '/swarm':
            if self.llm.swarm:
                print("\nğŸŒŠ Swarm Intelligence Status:")
                print(f"   Agents: {len(self.llm.swarm.agents)}")
                print(f"   Best Fitness: {self.llm.swarm.global_best_fitness:.3f}")
                print(f"   Total Optimizations: {self.llm.metrics['swarm_optimizations']}")
            else:
                print("âŒ Swarm intelligence disabled")
        
        elif cmd == '/kg':
            if self.llm.knowledge_graph:
                print("\nğŸ§© Knowledge Graph Status:")
                print(f"   Nodes: {len(self.llm.knowledge_graph.nodes)}")
                print(f"   Edges: {len(self.llm.knowledge_graph.edges)}")
                
                central = self.llm.knowledge_graph.get_central_nodes(5)
                if central:
                    print(f"\n   Central Nodes:")
                    for node_id, degree in central:
                        node = self.llm.knowledge_graph.nodes[node_id]
                        print(f"   â€¢ {node.name} (degree: {degree})")
            else:
                print("âŒ Knowledge graph disabled")
        
        elif cmd == '/help':
            self.print_welcome()
        
        else:
            print(f"âŒ Unknown command: {cmd}")
        
        return True
    
    def run(self):
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—"""
        self.print_welcome()
        
        while True:
            try:
                query = input("ğŸ‘¤ You: ").strip()
                
                if not query:
                    continue
                
                if query.startswith('/'):
                    if not self.handle_command(query):
                        break
                    continue
                
                print("\nâ³ Processing...")
                response = self.llm.query(query)
                
                self.history.append((query, response))
                self.print_response(response)
            
            except KeyboardInterrupt:
                print("\n\nâš ï¸  Interrupted. Type /exit to quit.")
                continue
            except EOFError:
                print("\nğŸ‘‹ Goodbye!")
                break
            except Exception as e:
                print(f"\nâŒ Error: {e}")
                logger.error(f"Chat error: {e}")


# ==================== ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ ====================

def main():
    """ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description='Quantum-Enhanced Self-Evolving LLM System v3.0Î³'
    )
    parser.add_argument('--model', default='llama-3.1-8b-instant', help='Base model')
    parser.add_argument('--no-quantum', action='store_true', help='Disable quantum')
    parser.add_argument('--no-genetic', action='store_true', help='Disable genetic')
    parser.add_argument('--no-swarm', action='store_true', help='Disable swarm')
    parser.add_argument('--no-rlhf', action='store_true', help='Disable RLHF')
    parser.add_argument('--query', type=str, help='Single query mode')
    parser.add_argument('--load', type=str, help='Load state')
    parser.add_argument('--debug', action='store_true', help='Debug mode')
    
    args = parser.parse_args()
    
    if args.debug:
        logger.logger.setLevel(logging.DEBUG)
    
    # è¨­å®š
    config = SystemConfig(
        model=args.model,
        quantum=QuantumConfig(enabled=not args.no_quantum),
        genetic=GeneticConfig(enabled=not args.no_genetic),
        swarm=SwarmConfig(enabled=not args.no_swarm),
        rlhf=RLHFConfig(enabled=not args.no_rlhf)
    )
    
    try:
        # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
        llm = QuantumLLM(config=config)
        
        # çŠ¶æ…‹èª­ã¿è¾¼ã¿
        if args.load:
            llm.load_state(args.load)
        
        # ã‚·ãƒ³ã‚°ãƒ«ã‚¯ã‚¨ãƒªãƒ¢ãƒ¼ãƒ‰
        if args.query:
            response = llm.query(args.query)
            print(response.text)
            print(f"\nğŸ“Š Metadata:")
            print(f"   Quality: {response.quality_score:.2f}")
            print(f"   Strategy: {response.strategy.value if response.strategy else 'N/A'}")
            print(f"   Cost: ${response.cost:.6f}")
            return
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰
        chat = QuantumChat(llm)
        chat.run()
        
        # çµ‚äº†æ™‚ä¿å­˜
        print("\nğŸ’¾ Saving session...")
        llm.save_state()
        
        stats = llm.get_stats()
        print("\nğŸ“Š Session Summary:")
        print(f"   Queries: {stats['system']['queries']}")
        print(f"   Success Rate: {stats['system']['success_rate']}")
        print(f"   Total Cost: {stats['system']['total_cost']}")
    
    except ValueError as e:
        print(f"\nâŒ Error: {e}")
        print("Please set GROQ_API_KEY environment variable")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ Fatal error: {e}")
        logger.error(f"Fatal: {e}")
        sys.exit(1)


if __name__ == '__main__':
    main()
